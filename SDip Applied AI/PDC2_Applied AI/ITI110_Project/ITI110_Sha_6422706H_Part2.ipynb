{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Load Dataset"
      ],
      "metadata": {
        "id": "8WG7-5Px7108"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchaudio transformers datasets librosa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NpC0evRh8Sry",
        "outputId": "c409fac3-3483-4ea3-bc9b-e03a19c5aeee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.3.2)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.61.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from transformers import Wav2Vec2Processor, Wav2Vec2ForSequenceClassification\n",
        "from datasets import Dataset\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "4_kwBMWu8Spa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "folder = '/content/drive/My Drive/ITI110'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSplhSv79HGU",
        "outputId": "82b2c203-faa3-40b7-d784-640046fa8b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the Dataset\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Define the path to the zip file in your Google Drive and where to unzip in Colab's virtual space\n",
        "zip_file_path = folder + '/RAVDESS_EmotionalSpeechAudio.zip'\n",
        "data_folder = '/content/RAVDESS/'  # Virtual Colab space (not on Google Drive)\n",
        "\n",
        "# Unzipping the dataset into the Colab virtual space\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(data_folder)\n",
        "\n",
        "# Now you can access the unzipped data in Colab\n",
        "print(\"Dataset extracted successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x908-BCN9G0h",
        "outputId": "7fe54802-f820-46f7-bd9f-15b666d7c815"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset extracted successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Explore Dataset Structure\n",
        "# run the following command to check the folder structure and see how files are organized\n",
        "\n",
        "#import os\n",
        "\n",
        "# List files and folders in the extracted dataset\n",
        "for root, dirs, files in os.walk(data_folder):\n",
        "    print(f\"ðŸ“‚ {root}\")\n",
        "    for file in files[:5]:  # Show only the first 5 files per folder\n",
        "        print(f\"  ðŸ“„ {file}\")\n",
        "    print(\"------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCJEOySk9GyE",
        "outputId": "45833cd5-ea78-4312-81bd-4aa3fb677b8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“‚ /content/RAVDESS/\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_16\n",
            "  ðŸ“„ 03-01-06-01-01-02-16.wav\n",
            "  ðŸ“„ 03-01-03-02-02-01-16.wav\n",
            "  ðŸ“„ 03-01-08-01-02-02-16.wav\n",
            "  ðŸ“„ 03-01-07-01-02-02-16.wav\n",
            "  ðŸ“„ 03-01-03-01-01-02-16.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_11\n",
            "  ðŸ“„ 03-01-07-02-02-01-11.wav\n",
            "  ðŸ“„ 03-01-08-02-01-01-11.wav\n",
            "  ðŸ“„ 03-01-02-01-02-02-11.wav\n",
            "  ðŸ“„ 03-01-04-01-01-01-11.wav\n",
            "  ðŸ“„ 03-01-06-02-01-01-11.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_20\n",
            "  ðŸ“„ 03-01-05-01-02-02-20.wav\n",
            "  ðŸ“„ 03-01-01-01-01-02-20.wav\n",
            "  ðŸ“„ 03-01-05-01-01-01-20.wav\n",
            "  ðŸ“„ 03-01-08-02-02-02-20.wav\n",
            "  ðŸ“„ 03-01-08-02-02-01-20.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_06\n",
            "  ðŸ“„ 03-01-07-02-02-01-06.wav\n",
            "  ðŸ“„ 03-01-08-02-01-02-06.wav\n",
            "  ðŸ“„ 03-01-03-02-02-02-06.wav\n",
            "  ðŸ“„ 03-01-06-02-02-02-06.wav\n",
            "  ðŸ“„ 03-01-08-01-02-01-06.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_10\n",
            "  ðŸ“„ 03-01-06-02-01-02-10.wav\n",
            "  ðŸ“„ 03-01-05-01-01-01-10.wav\n",
            "  ðŸ“„ 03-01-05-02-02-01-10.wav\n",
            "  ðŸ“„ 03-01-07-02-02-02-10.wav\n",
            "  ðŸ“„ 03-01-05-02-02-02-10.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_13\n",
            "  ðŸ“„ 03-01-05-01-01-02-13.wav\n",
            "  ðŸ“„ 03-01-08-01-02-02-13.wav\n",
            "  ðŸ“„ 03-01-04-01-01-01-13.wav\n",
            "  ðŸ“„ 03-01-01-01-01-01-13.wav\n",
            "  ðŸ“„ 03-01-02-02-01-01-13.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_21\n",
            "  ðŸ“„ 03-01-08-02-01-01-21.wav\n",
            "  ðŸ“„ 03-01-04-01-01-02-21.wav\n",
            "  ðŸ“„ 03-01-07-01-02-01-21.wav\n",
            "  ðŸ“„ 03-01-07-02-02-01-21.wav\n",
            "  ðŸ“„ 03-01-01-01-01-02-21.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_24\n",
            "  ðŸ“„ 03-01-04-02-02-02-24.wav\n",
            "  ðŸ“„ 03-01-07-02-01-02-24.wav\n",
            "  ðŸ“„ 03-01-02-02-02-01-24.wav\n",
            "  ðŸ“„ 03-01-02-01-02-01-24.wav\n",
            "  ðŸ“„ 03-01-04-02-01-01-24.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_23\n",
            "  ðŸ“„ 03-01-01-01-02-01-23.wav\n",
            "  ðŸ“„ 03-01-06-02-02-01-23.wav\n",
            "  ðŸ“„ 03-01-03-02-01-02-23.wav\n",
            "  ðŸ“„ 03-01-02-01-01-01-23.wav\n",
            "  ðŸ“„ 03-01-02-01-02-01-23.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_15\n",
            "  ðŸ“„ 03-01-07-01-02-02-15.wav\n",
            "  ðŸ“„ 03-01-06-01-01-02-15.wav\n",
            "  ðŸ“„ 03-01-08-01-01-02-15.wav\n",
            "  ðŸ“„ 03-01-08-01-02-02-15.wav\n",
            "  ðŸ“„ 03-01-08-02-01-02-15.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_07\n",
            "  ðŸ“„ 03-01-04-02-02-02-07.wav\n",
            "  ðŸ“„ 03-01-04-01-02-01-07.wav\n",
            "  ðŸ“„ 03-01-06-02-02-02-07.wav\n",
            "  ðŸ“„ 03-01-03-01-01-02-07.wav\n",
            "  ðŸ“„ 03-01-08-02-01-02-07.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_19\n",
            "  ðŸ“„ 03-01-05-01-01-01-19.wav\n",
            "  ðŸ“„ 03-01-05-02-01-01-19.wav\n",
            "  ðŸ“„ 03-01-08-02-02-02-19.wav\n",
            "  ðŸ“„ 03-01-06-02-01-01-19.wav\n",
            "  ðŸ“„ 03-01-04-01-02-01-19.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_22\n",
            "  ðŸ“„ 03-01-06-02-02-02-22.wav\n",
            "  ðŸ“„ 03-01-05-02-01-01-22.wav\n",
            "  ðŸ“„ 03-01-05-01-02-02-22.wav\n",
            "  ðŸ“„ 03-01-07-02-01-02-22.wav\n",
            "  ðŸ“„ 03-01-05-02-02-02-22.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_05\n",
            "  ðŸ“„ 03-01-03-01-01-01-05.wav\n",
            "  ðŸ“„ 03-01-08-02-02-02-05.wav\n",
            "  ðŸ“„ 03-01-08-01-02-01-05.wav\n",
            "  ðŸ“„ 03-01-07-01-01-02-05.wav\n",
            "  ðŸ“„ 03-01-08-02-02-01-05.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_12\n",
            "  ðŸ“„ 03-01-07-01-01-01-12.wav\n",
            "  ðŸ“„ 03-01-01-01-02-01-12.wav\n",
            "  ðŸ“„ 03-01-06-02-02-02-12.wav\n",
            "  ðŸ“„ 03-01-03-02-01-02-12.wav\n",
            "  ðŸ“„ 03-01-01-01-01-02-12.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_02\n",
            "  ðŸ“„ 03-01-06-01-01-02-02.wav\n",
            "  ðŸ“„ 03-01-07-02-02-02-02.wav\n",
            "  ðŸ“„ 03-01-07-01-02-01-02.wav\n",
            "  ðŸ“„ 03-01-07-02-01-01-02.wav\n",
            "  ðŸ“„ 03-01-08-02-02-01-02.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_04\n",
            "  ðŸ“„ 03-01-01-01-02-01-04.wav\n",
            "  ðŸ“„ 03-01-06-01-02-02-04.wav\n",
            "  ðŸ“„ 03-01-07-02-02-01-04.wav\n",
            "  ðŸ“„ 03-01-07-02-02-02-04.wav\n",
            "  ðŸ“„ 03-01-02-01-01-02-04.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_09\n",
            "  ðŸ“„ 03-01-02-02-02-01-09.wav\n",
            "  ðŸ“„ 03-01-06-02-01-01-09.wav\n",
            "  ðŸ“„ 03-01-06-02-02-01-09.wav\n",
            "  ðŸ“„ 03-01-01-01-02-02-09.wav\n",
            "  ðŸ“„ 03-01-06-02-01-02-09.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_18\n",
            "  ðŸ“„ 03-01-05-02-02-01-18.wav\n",
            "  ðŸ“„ 03-01-08-01-01-01-18.wav\n",
            "  ðŸ“„ 03-01-04-01-01-02-18.wav\n",
            "  ðŸ“„ 03-01-02-01-02-01-18.wav\n",
            "  ðŸ“„ 03-01-05-01-01-02-18.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_01\n",
            "  ðŸ“„ 03-01-04-02-02-02-01.wav\n",
            "  ðŸ“„ 03-01-05-02-02-01-01.wav\n",
            "  ðŸ“„ 03-01-07-01-02-02-01.wav\n",
            "  ðŸ“„ 03-01-02-02-02-01-01.wav\n",
            "  ðŸ“„ 03-01-07-02-02-01-01.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_08\n",
            "  ðŸ“„ 03-01-03-01-01-01-08.wav\n",
            "  ðŸ“„ 03-01-03-01-02-02-08.wav\n",
            "  ðŸ“„ 03-01-03-01-01-02-08.wav\n",
            "  ðŸ“„ 03-01-02-02-01-01-08.wav\n",
            "  ðŸ“„ 03-01-04-01-01-01-08.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_03\n",
            "  ðŸ“„ 03-01-08-01-01-02-03.wav\n",
            "  ðŸ“„ 03-01-02-01-01-02-03.wav\n",
            "  ðŸ“„ 03-01-01-01-01-01-03.wav\n",
            "  ðŸ“„ 03-01-05-01-01-02-03.wav\n",
            "  ðŸ“„ 03-01-04-02-02-02-03.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_14\n",
            "  ðŸ“„ 03-01-08-02-02-01-14.wav\n",
            "  ðŸ“„ 03-01-03-02-02-02-14.wav\n",
            "  ðŸ“„ 03-01-06-01-02-01-14.wav\n",
            "  ðŸ“„ 03-01-08-01-01-02-14.wav\n",
            "  ðŸ“„ 03-01-08-01-02-01-14.wav\n",
            "------\n",
            "ðŸ“‚ /content/RAVDESS/Actor_17\n",
            "  ðŸ“„ 03-01-06-02-02-02-17.wav\n",
            "  ðŸ“„ 03-01-01-01-02-01-17.wav\n",
            "  ðŸ“„ 03-01-04-02-02-01-17.wav\n",
            "  ðŸ“„ 03-01-08-01-02-01-17.wav\n",
            "  ðŸ“„ 03-01-06-01-01-02-17.wav\n",
            "------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import os\n",
        "#import pandas as pd\n",
        "import glob\n",
        "\n",
        "# Define dataset folder\n",
        "data_folder = \"/content/RAVDESS/\"\n",
        "\n",
        "# Extract all WAV file paths\n",
        "file_paths = glob.glob(os.path.join(data_folder, \"Actor_*\", \"*.wav\"))\n"
      ],
      "metadata": {
        "id": "FSj9tIjX8Sm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Emotion mapping from filename convention\n",
        "emotion_map = {\n",
        "    \"01\": \"neutral\",\n",
        "    \"02\": \"calm\",\n",
        "    \"03\": \"happy\",\n",
        "    \"04\": \"sad\",\n",
        "    \"05\": \"angry\",\n",
        "    \"06\": \"fearful\",\n",
        "    \"07\": \"disgust\",\n",
        "    \"08\": \"surprised\"\n",
        "}\n",
        "\n",
        "# Function to extract emotion & gender from filename\n",
        "def parse_filename(filepath):\n",
        "    filename = os.path.basename(filepath)\n",
        "    parts = filename.split(\"-\")  # Split filename using '-'\n",
        "\n",
        "    if len(parts) > 2:\n",
        "        emotion_code = parts[2]  # 3rd part of filename is the emotion code\n",
        "        emotion = emotion_map.get(emotion_code, \"unknown\")\n",
        "\n",
        "        actor_id = parts[-1].split(\".\")[0]  # Last part before `.wav` is the actor ID\n",
        "        gender = \"male\" if int(actor_id) % 2 != 0 else \"female\"\n",
        "\n",
        "        return emotion, gender\n",
        "    return \"unknown\", \"unknown\"\n"
      ],
      "metadata": {
        "id": "jLzGoxIx8Sks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame with file paths only\n",
        "df = pd.DataFrame({\n",
        "    \"File Name\": [os.path.basename(f) for f in file_paths],  # Extract just filenames\n",
        "    \"File Path\": file_paths  # Full path for loading\n",
        "})\n",
        "\n",
        "# Apply function to extract emotion & gender\n",
        "df[[\"Emotion\", \"Gender\"]] = df[\"File Path\"].apply(lambda f: pd.Series(parse_filename(f)))\n",
        "\n",
        "# Display sample\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "IVWVCA_f8SiI",
        "outputId": "0435f8dc-d928-47ed-de83-6acd177e3a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                  File Name  \\\n",
              "0  03-01-06-01-01-02-16.wav   \n",
              "1  03-01-03-02-02-01-16.wav   \n",
              "2  03-01-08-01-02-02-16.wav   \n",
              "3  03-01-07-01-02-02-16.wav   \n",
              "4  03-01-03-01-01-02-16.wav   \n",
              "\n",
              "                                           File Path    Emotion  Gender  \n",
              "0  /content/RAVDESS/Actor_16/03-01-06-01-01-02-16...    fearful  female  \n",
              "1  /content/RAVDESS/Actor_16/03-01-03-02-02-01-16...      happy  female  \n",
              "2  /content/RAVDESS/Actor_16/03-01-08-01-02-02-16...  surprised  female  \n",
              "3  /content/RAVDESS/Actor_16/03-01-07-01-02-02-16...    disgust  female  \n",
              "4  /content/RAVDESS/Actor_16/03-01-03-01-01-02-16...      happy  female  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-50f9ef95-a774-4757-bfc3-47681c686707\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>File Name</th>\n",
              "      <th>File Path</th>\n",
              "      <th>Emotion</th>\n",
              "      <th>Gender</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>03-01-06-01-01-02-16.wav</td>\n",
              "      <td>/content/RAVDESS/Actor_16/03-01-06-01-01-02-16...</td>\n",
              "      <td>fearful</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>03-01-03-02-02-01-16.wav</td>\n",
              "      <td>/content/RAVDESS/Actor_16/03-01-03-02-02-01-16...</td>\n",
              "      <td>happy</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>03-01-08-01-02-02-16.wav</td>\n",
              "      <td>/content/RAVDESS/Actor_16/03-01-08-01-02-02-16...</td>\n",
              "      <td>surprised</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>03-01-07-01-02-02-16.wav</td>\n",
              "      <td>/content/RAVDESS/Actor_16/03-01-07-01-02-02-16...</td>\n",
              "      <td>disgust</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>03-01-03-01-01-02-16.wav</td>\n",
              "      <td>/content/RAVDESS/Actor_16/03-01-03-01-01-02-16...</td>\n",
              "      <td>happy</td>\n",
              "      <td>female</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50f9ef95-a774-4757-bfc3-47681c686707')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-50f9ef95-a774-4757-bfc3-47681c686707 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-50f9ef95-a774-4757-bfc3-47681c686707');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ff8754d4-72fc-4909-a5d6-77a83afc6496\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ff8754d4-72fc-4909-a5d6-77a83afc6496')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ff8754d4-72fc-4909-a5d6-77a83afc6496 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 1440,\n  \"fields\": [\n    {\n      \"column\": \"File Name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1440,\n        \"samples\": [\n          \"03-01-02-01-02-02-20.wav\",\n          \"03-01-07-01-01-02-07.wav\",\n          \"03-01-05-01-01-02-15.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"File Path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1440,\n        \"samples\": [\n          \"/content/RAVDESS/Actor_20/03-01-02-01-02-02-20.wav\",\n          \"/content/RAVDESS/Actor_07/03-01-07-01-01-02-07.wav\",\n          \"/content/RAVDESS/Actor_15/03-01-05-01-01-02-15.wav\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Emotion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"happy\",\n          \"calm\",\n          \"fearful\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Gender\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"male\",\n          \"female\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nTotal Samples: {len(df)}\")\n",
        "\n",
        "# Count of each emotion\n",
        "print(\"\\nEmotion Distribution:\")\n",
        "print(df[\"Emotion\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nHBetPt4-mNa",
        "outputId": "2a4b90d2-0da3-45ed-f5e0-cff6527f5fda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total Samples: 1440\n",
            "\n",
            "Emotion Distribution:\n",
            "Emotion\n",
            "fearful      192\n",
            "happy        192\n",
            "surprised    192\n",
            "disgust      192\n",
            "sad          192\n",
            "calm         192\n",
            "angry        192\n",
            "neutral       96\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Preprocessing steps"
      ],
      "metadata": {
        "id": "6i-sVKWfFSXv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "\n",
        "# Define the target sample rate\n",
        "target_sample_rate = 16000\n",
        "\n",
        "# Function to load and preprocess audio (including resampling and normalization)\n",
        "def load_audio(filepath):\n",
        "    # Load the audio file\n",
        "    waveform, sample_rate = torchaudio.load(filepath)\n",
        "\n",
        "    # Resample if the sample rate is different from the target\n",
        "    if sample_rate != target_sample_rate:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    # Normalize the audio\n",
        "    waveform = waveform / waveform.abs().max()  # Normalize to -1 to 1\n",
        "\n",
        "    return waveform\n",
        "\n",
        "# Function to trim silence from the audio\n",
        "def trim_silence(waveform, threshold=0.01):\n",
        "    # Use torchaudio's VAD (Voice Activity Detection) to trim silence\n",
        "    vad = T.Vad(sample_rate=target_sample_rate, trigger_level=threshold)\n",
        "    trimmed_waveform = vad(waveform)\n",
        "    return trimmed_waveform\n",
        "\n",
        "# Example usage: trim silence from one audio file\n",
        "audio_file = file_paths[0]  # Select one of the audio file paths\n",
        "waveform = load_audio(audio_file)  # Load the audio\n",
        "trimmed_waveform = trim_silence(waveform)  # Trim silence\n",
        "\n",
        "# Display original vs trimmed audio length\n",
        "print(f\"Original audio length: {waveform.shape[1]} samples\")\n",
        "print(f\"Trimmed audio length: {trimmed_waveform.shape[1]} samples\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlt2C7Tb-mK5",
        "outputId": "daffc366-7655-4baf-a652-d4e3e0e17695"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original audio length: 56056 samples\n",
            "Trimmed audio length: 41656 samples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Oversampling of Neutral Samples\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Separate the neutral emotion class\n",
        "neutral_df = df[df[\"Emotion\"] == \"neutral\"]\n",
        "\n",
        "# Separate the majority classes (those that aren't neutral)\n",
        "majority_df = df[df[\"Emotion\"] != \"neutral\"]\n",
        "\n",
        "# Oversample the neutral class to 192 samples (matching the other classes)\n",
        "neutral_oversampled = resample(neutral_df,\n",
        "                               replace=True,      # Sample with replacement\n",
        "                               n_samples=192,     # Set to 192 to match the other classes\n",
        "                               random_state=42)   # For reproducibility\n",
        "\n",
        "# Concatenate the oversampled neutral class back with the majority class data\n",
        "df_balanced = pd.concat([majority_df, neutral_oversampled])\n",
        "\n",
        "# Check the new emotion distribution\n",
        "print(\"\\nBalanced Emotion Distribution:\")\n",
        "print(df_balanced[\"Emotion\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCwj8lz5K5wk",
        "outputId": "a01512e4-0f8b-4a48-fb88-622787e06502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Balanced Emotion Distribution:\n",
            "Emotion\n",
            "fearful      192\n",
            "happy        192\n",
            "surprised    192\n",
            "disgust      192\n",
            "sad          192\n",
            "calm         192\n",
            "angry        192\n",
            "neutral      192\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Map Labels to Emotion Groups\n",
        "\n",
        "# Define the emotion mapping\n",
        "emotion_mapping = {\n",
        "    \"neutral\": \"neutral\",\n",
        "    \"calm\": \"neutral\",\n",
        "    \"happy\": \"positive\",\n",
        "    \"surprised\": \"positive\",\n",
        "    \"sad\": \"negative\",\n",
        "    \"disgust\": \"negative\",\n",
        "    \"angry\": \"negative\",\n",
        "    \"fearful\": \"negative\"\n",
        "}\n",
        "\n",
        "# Apply the mapping to create a new column 'Emotion Group'\n",
        "df_balanced['Label'] = df_balanced['Emotion'].map(emotion_mapping)\n",
        "\n",
        "# Display the first few rows to check the mapping\n",
        "print(df_balanced.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SkFgrXL-mIf",
        "outputId": "9ffb700a-fe6b-4c31-9575-6e808f159e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                  File Name  \\\n",
            "0  03-01-06-01-01-02-16.wav   \n",
            "1  03-01-03-02-02-01-16.wav   \n",
            "2  03-01-08-01-02-02-16.wav   \n",
            "3  03-01-07-01-02-02-16.wav   \n",
            "4  03-01-03-01-01-02-16.wav   \n",
            "\n",
            "                                           File Path    Emotion  Gender  \\\n",
            "0  /content/RAVDESS/Actor_16/03-01-06-01-01-02-16...    fearful  female   \n",
            "1  /content/RAVDESS/Actor_16/03-01-03-02-02-01-16...      happy  female   \n",
            "2  /content/RAVDESS/Actor_16/03-01-08-01-02-02-16...  surprised  female   \n",
            "3  /content/RAVDESS/Actor_16/03-01-07-01-02-02-16...    disgust  female   \n",
            "4  /content/RAVDESS/Actor_16/03-01-03-01-01-02-16...      happy  female   \n",
            "\n",
            "      Label  \n",
            "0  negative  \n",
            "1  positive  \n",
            "2  positive  \n",
            "3  negative  \n",
            "4  positive  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nTotal Samples: {len(df_balanced)}\")\n",
        "\n",
        "# Count of each emotion\n",
        "print(\"\\nLabel Distribution:\")\n",
        "print(df_balanced[\"Label\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSzWc4oDNSr6",
        "outputId": "670d3340-8257-4e60-df07-ed37a9a5471d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total Samples: 1536\n",
            "\n",
            "Label Distribution:\n",
            "Label\n",
            "negative    768\n",
            "positive    384\n",
            "neutral     384\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified Undersampling for Negative Label\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Separate the negative, positive, and neutral classes\n",
        "negative_df = df_balanced[df_balanced[\"Label\"] == \"negative\"]\n",
        "positive_neutral_df = df_balanced[df_balanced[\"Label\"] != \"negative\"]\n",
        "\n",
        "# Perform stratified undersampling for the negative class to 384 samples\n",
        "negative_undersampled = resample(negative_df,\n",
        "                                 replace=False,      # Sample without replacement\n",
        "                                 n_samples=384,      # Set to 384 to match the other classes\n",
        "                                 random_state=42)    # For reproducibility\n",
        "\n",
        "# Concatenate the undersampled negative class back with the other classes\n",
        "df_balMap = pd.concat([positive_neutral_df, negative_undersampled])\n",
        "\n",
        "# Check the new label distribution\n",
        "print(\"\\nBalanced Label Distribution after Undersampling:\")\n",
        "print(df_balMap[\"Label\"].value_counts())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2N-b4QRM-mGS",
        "outputId": "8cd48374-6307-4e3c-943e-d991744c0c2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Balanced Label Distribution after Undersampling:\n",
            "Label\n",
            "positive    384\n",
            "neutral     384\n",
            "negative    384\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Resample to 16kHz & Padding/Trimming to 2 seconds\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "\n",
        "# Function to resample audio to 16kHz\n",
        "def resample_audio(audio_path, target_sample_rate=16000):\n",
        "    waveform, original_sample_rate = torchaudio.load(audio_path)\n",
        "    if original_sample_rate != target_sample_rate:\n",
        "        resampler = torchaudio.transforms.Resample(orig_freq=original_sample_rate, new_freq=target_sample_rate)\n",
        "        waveform = resampler(waveform)\n",
        "    return waveform\n",
        "\n",
        "# Function to pad or trim the audio to 2 seconds (at 16kHz)\n",
        "def pad_or_trim_audio(waveform, target_duration_sec=2, sample_rate=16000):\n",
        "    target_length = target_duration_sec * sample_rate\n",
        "    waveform_length = waveform.size(1)\n",
        "\n",
        "    if waveform_length > target_length:  # Trim the waveform\n",
        "        waveform = waveform[:, :target_length]\n",
        "    elif waveform_length < target_length:  # Pad the waveform\n",
        "        padding = target_length - waveform_length\n",
        "        waveform = torch.nn.functional.pad(waveform, (0, padding))  # Pad with zeros at the end\n",
        "\n",
        "    return waveform\n",
        "\n",
        "# Example usage on your dataset\n",
        "def preprocess_audio_file(audio_path):\n",
        "    # Resample to 16kHz\n",
        "    resampled_waveform = resample_audio(audio_path)\n",
        "\n",
        "    # Pad or trim to 2 seconds\n",
        "    final_waveform = pad_or_trim_audio(resampled_waveform)\n",
        "\n",
        "    return final_waveform\n",
        "\n",
        "# Example: Apply preprocessing to all audio files in your DataFrame\n",
        "df_balMap['Processed Audio'] = df_balMap['File Path'].apply(lambda x: preprocess_audio_file(x))\n",
        "\n",
        "# Verify the preprocessing works by checking the shape of one waveform\n",
        "print(f\"Processed audio shape for first file: {df_balMap['Processed Audio'].iloc[0].shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3s-12WhI-mD9",
        "outputId": "881dac22-c5a4-4df7-818a-fef32877e4ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed audio shape for first file: torch.Size([1, 32000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 Preparing the Dataset for Wave2Vec"
      ],
      "metadata": {
        "id": "AMGK7dwIUBzT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Preparing the Dataset for Wave2Vec\n",
        "Will need to create a custom dataset class that will handle the loading of your audio files, along with their corresponding labels (emotion labels).\n",
        "\n",
        "\n",
        "1.   Audio Input: The input to Wave2Vec should be the waveform (as a tensor).\n",
        "2.   Labels: The labels for emotion classification will be the Label column in your df_balMap DataFrame.\n",
        "\n",
        "\n",
        "Step 2: Implement Dataset Class\n",
        "We'll use PyTorch's Dataset class to create a custom dataset that loads the audio and the corresponding labels."
      ],
      "metadata": {
        "id": "amwhVyL_UPra"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Dataset Class\n",
        "\n",
        "# import torch\n",
        "# from torch.utils.data import Dataset\n",
        "# import torchaudio\n",
        "\n",
        "# class EmotionDataset(Dataset):\n",
        "#     def __init__(self, dataframe, transform=None):\n",
        "#         self.dataframe = dataframe\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.dataframe)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # Get the file path and label\n",
        "#         audio_path = self.dataframe.iloc[idx]['File Path']\n",
        "#         label = self.dataframe.iloc[idx]['Label']\n",
        "\n",
        "#         # Load the audio file\n",
        "#         waveform = self.dataframe.iloc[idx]['Processed Audio']  # Already processed audio\n",
        "\n",
        "#         # Convert the label to an integer\n",
        "#         label_map = {'negative': 0, 'positive': 1, 'neutral': 2}  # Map your labels to integers\n",
        "#         label_idx = label_map[label]\n",
        "\n",
        "#         # Apply any transformations (if needed)\n",
        "#         if self.transform:\n",
        "#             waveform = self.transform(waveform)\n",
        "\n",
        "#         return waveform, label_idx\n",
        "\n",
        "# # Example: Create the dataset for training\n",
        "# emotion_dataset = EmotionDataset(df_balMap)\n",
        "\n",
        "# # Example: Get the first item from the dataset\n",
        "# example_waveform, example_label = emotion_dataset[0]\n",
        "# print(f\"Waveform shape: {example_waveform.shape}, Label: {example_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Isir_wxW-mBx",
        "outputId": "5f0241b9-f938-4251-e9a1-a2330645b09a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waveform shape: torch.Size([1, 32000]), Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# from torch.utils.data import Dataset\n",
        "# import torchaudio\n",
        "\n",
        "# class EmotionDataset(Dataset):\n",
        "#     def __init__(self, dataframe, transform=None):\n",
        "#         self.dataframe = dataframe\n",
        "#         self.transform = transform\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.dataframe)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         # Get the file path and label\n",
        "#         audio_path = self.dataframe.iloc[idx]['File Path']\n",
        "#         label = self.dataframe.iloc[idx]['Label']\n",
        "\n",
        "#         # Load the processed audio directly\n",
        "#         waveform = self.dataframe.iloc[idx]['Processed Audio']  # Already processed audio\n",
        "\n",
        "#         # Map label to integer\n",
        "#         label_map = {'negative': 0, 'positive': 1, 'neutral': 2}  # Label mapping\n",
        "#         label_idx = torch.tensor(label_map[label], dtype=torch.long)  # Convert label to tensor\n",
        "\n",
        "#         # Apply transformations (if needed)\n",
        "#         if self.transform:\n",
        "#             waveform = self.transform(waveform)\n",
        "\n",
        "#         return waveform, label_idx  # Ensure label is returned as a tensor\n",
        "\n",
        "# # Example: Create the dataset for training\n",
        "# emotion_dataset = EmotionDataset(df_balMap)\n",
        "\n",
        "# # Example: Get the first item from the dataset\n",
        "# example_waveform, example_label = emotion_dataset[0]\n",
        "# print(f\"Waveform shape: {example_waveform.shape}, Label: {example_label}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLdgKeZPM2nx",
        "outputId": "c3aa7c6c-f8ed-4e59-953d-7abbc72a615e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waveform shape: torch.Size([1, 32000]), Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, dataframe, processor, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.processor = processor  # Wav2Vec2 processor\n",
        "        self.transform = transform\n",
        "        self.label_map = {'negative': 0, 'positive': 1, 'neutral': 2}  # Label mapping\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load preprocessed audio from DataFrame\n",
        "        #waveform = self.dataframe.iloc[idx]['Processed Audio']  # Already processed audio\n",
        "        waveform = torch.tensor(self.dataframe.iloc[idx]['Processed Audio'], dtype=torch.float32)\n",
        "        label = self.dataframe.iloc[idx]['Label']\n",
        "\n",
        "        # Convert label to integer\n",
        "        label_idx = torch.tensor(self.label_map[label], dtype=torch.long)\n",
        "\n",
        "        # Process audio with Wav2Vec2 processor\n",
        "        inputs = self.processor(waveform.numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "        #inputs = self.processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
        "        input_values = inputs.input_values.squeeze(0)  # Remove batch dimension\n",
        "\n",
        "        return input_values, label_idx  # Ensure label is returned as a tensor\n"
      ],
      "metadata": {
        "id": "FLlyevapPCce"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Creating DataLoader\n",
        "Once we have the EmotionDataset class, we can use PyTorchâ€™s DataLoader to handle batching, shuffling, and parallel processing of the data."
      ],
      "metadata": {
        "id": "RnE3NNgCUzLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Split Data into Train/Validation (70:30)\n",
        "train_df, val_df = train_test_split(df_balMap, test_size=0.3, stratify=df_balMap['Label'], random_state=42)\n",
        "print(f\"Training set size: {len(train_df)}\")\n",
        "print(f\"Validation set size: {len(val_df)}\")"
      ],
      "metadata": {
        "id": "nCY0Dlu6WHyD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c03bef-0a6e-462a-9d34-6942eb0eaea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 806\n",
            "Validation set size: 346\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import torch\n",
        "#import os\n",
        "#from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
        "from torch import nn\n",
        "from transformers import AdamW\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "#import numpy as np"
      ],
      "metadata": {
        "id": "7WyrLV55WH0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Load Processor and Model\n",
        "model_name = \"facebook/wav2vec2-base\"\n",
        "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 labels: negative, positive, neutral\n",
        "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ],
      "metadata": {
        "id": "G_jezFKbWHpz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b696ccc-13ac-4ea6-a48d-bfc9e6c488cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/configuration_utils.py:312: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
            "  warnings.warn(\n",
            "Some weights of Wav2Vec2ForSequenceClassification were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'projector.bias', 'projector.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Wav2Vec2ForSequenceClassification(\n",
              "  (wav2vec2): Wav2Vec2Model(\n",
              "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
              "      (conv_layers): ModuleList(\n",
              "        (0): Wav2Vec2GroupNormConvLayer(\n",
              "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,), bias=False)\n",
              "          (activation): GELUActivation()\n",
              "          (layer_norm): GroupNorm(512, 512, eps=1e-05, affine=True)\n",
              "        )\n",
              "        (1-4): 4 x Wav2Vec2NoLayerNormConvLayer(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (5-6): 2 x Wav2Vec2NoLayerNormConvLayer(\n",
              "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,), bias=False)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (feature_projection): Wav2Vec2FeatureProjection(\n",
              "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (projection): Linear(in_features=512, out_features=768, bias=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): Wav2Vec2Encoder(\n",
              "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
              "        (conv): ParametrizedConv1d(\n",
              "          768, 768, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
              "          (parametrizations): ModuleDict(\n",
              "            (weight): ParametrizationList(\n",
              "              (0): _WeightNorm()\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "        (padding): Wav2Vec2SamePadLayer()\n",
              "        (activation): GELUActivation()\n",
              "      )\n",
              "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x Wav2Vec2EncoderLayer(\n",
              "          (attention): Wav2Vec2SdpaAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (feed_forward): Wav2Vec2FeedForward(\n",
              "            (intermediate_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (intermediate_dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "            (output_dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (output_dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (projector): Linear(in_features=768, out_features=256, bias=True)\n",
              "  (classifier): Linear(in_features=256, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataloader import default_collate\n",
        "\n",
        "# 4. Create DataLoader for Train and Validation\n",
        "train_dataset = EmotionDataset(train_df, processor)\n",
        "val_dataset = EmotionDataset(val_df, processor)\n",
        "\n",
        "# def custom_collate_fn(batch):\n",
        "#     # Ensure all waveforms have the same number of channels (1 channel in this case)\n",
        "#     waveforms, labels = zip(*batch)\n",
        "\n",
        "#     # Process each waveform in the batch to ensure consistent shape\n",
        "#     processed_waveforms = []\n",
        "#     for waveform in waveforms:\n",
        "#         # If the waveform has more than 1 channel, convert it to mono by averaging over the channels\n",
        "#         if waveform.ndim > 1:\n",
        "#             waveform = waveform.mean(axis=0)  # Averaging channels to make it mono\n",
        "\n",
        "#         # Ensure the waveform is a 1D tensor (shape: [num_samples]) and convert to tensor\n",
        "#         waveform = torch.tensor(waveform, dtype=torch.float32)\n",
        "\n",
        "#         # Add to the list of processed waveforms\n",
        "#         processed_waveforms.append(waveform)\n",
        "\n",
        "#     # Use the default collate_fn to stack the processed waveforms into a batch\n",
        "#     waveforms_batch = default_collate(processed_waveforms)\n",
        "#     labels_batch = torch.tensor(labels)\n",
        "\n",
        "#     return waveforms_batch, labels_batch\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "# train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, collate_fn=custom_collate_fn)\n",
        "# val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=32, collate_fn=custom_collate_fn)\n"
      ],
      "metadata": {
        "id": "WATSK-vrWHn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from torch.utils.data import DataLoader\n",
        "\n",
        "# Create DataLoader for training\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "# Example: Get a batch of data\n",
        "waveforms, labels = next(iter(train_dataloader))\n",
        "print(f\"Batch of waveforms shape: {waveforms.shape}, Labels shape: {labels.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z4mmk97W-l_p",
        "outputId": "0328ac6d-7f7b-4942-a766-6c462c6f50df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch of waveforms shape: torch.Size([32, 32000]), Labels shape: torch.Size([32])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-c31dac6cb5c4>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  waveform = torch.tensor(self.dataframe.iloc[idx]['Processed Audio'], dtype=torch.float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "\n",
        "1.   EmotionDataset Class:\n",
        "  *   Loads the processed audio and corresponding label.\n",
        "  *   Converts the label to an integer (for training with Wave2Vec).\n",
        "  *   Supports optional transformations if needed.\n",
        "\n",
        "2.   DataLoader:\n",
        "  *   Batches the dataset for efficient processing.\n",
        "  *   Shuffles the data to ensure randomness during training.\n",
        "  *   Allows parallel data loading using multiple workers.\n"
      ],
      "metadata": {
        "id": "WBXA68UWU8w7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 Fine-tuning Wave2Vec 2.0\n",
        "To fine-tune Wave2Vec 2.0 on your emotion classification task, youâ€™ll need to:\n",
        "\n",
        "\n",
        "1.   Load Pre-trained Wave2Vec 2.0: We'll load the pre-trained model from the Hugging Face transformers library.\n",
        "2.   Modify the Model for Emotion Classification: Wave2Vec 2.0 outputs embeddings, and we'll add a classification head (e.g., a simple fully connected layer) on top of it to predict the emotion labels.\n",
        "3.   Train the Model: Train the model on your dataset for emotion classification.\n"
      ],
      "metadata": {
        "id": "RWMyK6fLWK4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Load Pre-trained Wave2Vec 2.0 Model\n",
        "\n",
        "# import torch\n",
        "# from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor\n",
        "# from torch import nn\n",
        "# from transformers import AdamW\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# # Load the pre-trained Wav2Vec 2.0 model and processor\n",
        "# #model_name = \"facebook/wav2vec2-base\"\n",
        "# model_name = \"facebook/wav2vec2-small\"\n",
        "# model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 labels: negative, positive, neutral\n",
        "# processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
        "\n",
        "# # Move model to GPU if available\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model.to(device)\n",
        "\n",
        "# # Define loss function and optimizer\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "# optimizer = AdamW(model.parameters(), lr=1e-5)\n",
        "\n",
        "# # Training loop\n",
        "# def train_model(model, dataloader, optimizer, loss_fn, epochs=3):\n",
        "#     model.train()  # Set the model to training mode\n",
        "#     for epoch in range(epochs):\n",
        "#         total_loss = 0\n",
        "#         for batch in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
        "#             input_waveforms, labels = batch\n",
        "#             input_waveforms = input_waveforms.squeeze(1).to(device)  # Remove channel dimension and move to device\n",
        "#             labels = labels.to(device)\n",
        "\n",
        "#             # Process the audio to the correct input format for Wave2Vec 2.0\n",
        "#             input_values = processor(input_waveforms, return_tensors=\"pt\", padding=True).input_values\n",
        "#             input_values = input_values.to(device)\n",
        "\n",
        "#             # Forward pass\n",
        "#             outputs = model(input_values, labels=labels)\n",
        "#             loss = outputs.loss\n",
        "#             logits = outputs.logits\n",
        "\n",
        "#             # Backward pass\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             total_loss += loss.item()\n",
        "\n",
        "#         print(f\"Epoch {epoch + 1} Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "# # Train the model\n",
        "# train_model(model, train_dataloader, optimizer, loss_fn, epochs=3)\n"
      ],
      "metadata": {
        "id": "9sBEaVlJ-l9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 2. Custom Dataset Class for Audio Files\n",
        "# class AudioDataset(Dataset):\n",
        "#     def __init__(self, dataframe, processor, max_duration=2, sampling_rate=16000):\n",
        "#         self.dataframe = dataframe\n",
        "#         self.processor = processor\n",
        "#         self.max_duration = max_duration\n",
        "#         self.sampling_rate = sampling_rate\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.dataframe)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         file_path = self.dataframe.iloc[idx]['File Path']\n",
        "#         label = self.dataframe.iloc[idx]['Label']\n",
        "\n",
        "#         # Load and preprocess audio\n",
        "#         waveform, _ = torchaudio.load(file_path)\n",
        "#         waveform = self.resample_audio(waveform)\n",
        "#         waveform = self.trim_or_pad(waveform)\n",
        "\n",
        "#         # Process audio with Wav2Vec2 processor\n",
        "#         inputs = self.processor(waveform, return_tensors=\"pt\", sampling_rate=self.sampling_rate, padding=True)\n",
        "#         input_values = inputs.input_values.squeeze(0)  # Remove the batch dimension\n",
        "\n",
        "#         return input_values, label\n",
        "\n",
        "#     def resample_audio(self, waveform):\n",
        "#         return torchaudio.transforms.Resample(orig_freq=44100, new_freq=self.sampling_rate)(waveform)\n",
        "\n",
        "#     def trim_or_pad(self, waveform):\n",
        "#         target_length = self.max_duration * self.sampling_rate\n",
        "#         current_length = waveform.shape[1]\n",
        "\n",
        "#         if current_length > target_length:\n",
        "#             return waveform[:, :target_length]\n",
        "#         elif current_length < target_length:\n",
        "#             padding = target_length - current_length\n",
        "#             return torch.nn.functional.pad(waveform, (0, padding))\n",
        "#         else:\n",
        "#             return waveform\n"
      ],
      "metadata": {
        "id": "jL2QnDlGWHrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 2. Custom Dataset Class for Audio Files\n",
        "# class AudioDataset(Dataset):\n",
        "#     def __init__(self, dataframe, processor, max_duration=2, sampling_rate=16000):\n",
        "#         self.dataframe = dataframe\n",
        "#         self.processor = processor\n",
        "#         self.max_duration = max_duration\n",
        "#         self.sampling_rate = sampling_rate\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.dataframe)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         file_path = self.dataframe.iloc[idx]['File Path']\n",
        "#         label = torch.tensor(self.dataframe.iloc[idx]['Label'], dtype=torch.long)  # Convert label to tensor\n",
        "\n",
        "#         # Load and preprocess audio\n",
        "#         waveform, _ = torchaudio.load(file_path)\n",
        "#         waveform = self.resample_audio(waveform)\n",
        "#         waveform = self.trim_or_pad(waveform)\n",
        "\n",
        "#         # Process audio with Wav2Vec2 processor\n",
        "#         inputs = self.processor(waveform, return_tensors=\"pt\", sampling_rate=self.sampling_rate, padding=True)\n",
        "#         input_values = inputs.input_values.squeeze(0)  # Remove the batch dimension\n",
        "\n",
        "#         return input_values, label  # Ensure label is a tensor\n",
        "\n",
        "#     def resample_audio(self, waveform):\n",
        "#         return torchaudio.transforms.Resample(orig_freq=44100, new_freq=self.sampling_rate)(waveform)\n",
        "\n",
        "#     def trim_or_pad(self, waveform):\n",
        "#         target_length = self.max_duration * self.sampling_rate\n",
        "#         current_length = waveform.shape[1]\n",
        "\n",
        "#         if current_length > target_length:\n",
        "#             return waveform[:, :target_length]\n",
        "#         elif current_length < target_length:\n",
        "#             padding = target_length - current_length\n",
        "#             return torch.nn.functional.pad(waveform, (0, padding))\n",
        "#         else:\n",
        "#             return waveform\n"
      ],
      "metadata": {
        "id": "bAsuRAWnLRPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1 Define Callbacks\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, delta=0):\n",
        "        self.patience = patience\n",
        "        self.delta = delta\n",
        "        self.best_loss = float('inf')\n",
        "        self.counter = 0\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, loss):\n",
        "        if loss < self.best_loss - self.delta:\n",
        "            self.best_loss = loss\n",
        "            self.counter = 0\n",
        "        else:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "\n",
        "# Early stopping and model checkpoint\n",
        "early_stopping = EarlyStopping(patience=5)\n",
        "\n",
        "def save_best_model(model, epoch, loss, path='./'):\n",
        "    model_save_path = os.path.join(path, f'model_epoch_{epoch + 1}_loss_{loss:.4f}.pth')\n",
        "    torch.save(model.state_dict(), model_save_path)\n",
        "    print(f\"Model saved at {model_save_path}\")\n"
      ],
      "metadata": {
        "id": "N-4LzrkEkycp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 5.2 Define Training Loop\n",
        "\n",
        "# # Optimizer and loss function\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "# loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# # Training function\n",
        "# def train_model(model, train_dataloader, val_dataloader, optimizer, loss_fn, epochs=3):\n",
        "#     model.train()\n",
        "#     best_val_loss = float('inf')\n",
        "#     training_losses = []\n",
        "#     validation_losses = []\n",
        "#     train_accuracies = []\n",
        "#     val_accuracies = []\n",
        "\n",
        "#     for epoch in range(epochs):\n",
        "#         total_loss = 0\n",
        "#         correct_preds_train = 0\n",
        "#         total_train = 0\n",
        "#         for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
        "#             input_waveforms, labels = batch\n",
        "#             input_waveforms = input_waveforms.squeeze(1).to(device)\n",
        "#             labels = torch.tensor(labels, dtype=torch.long).to(device)  # Ensure labels are tensors\n",
        "\n",
        "#         # for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
        "#         #     input_waveforms, labels = batch\n",
        "#         #     input_waveforms = input_waveforms.squeeze(1).to(device)\n",
        "#         #     labels = labels.to(device)\n",
        "\n",
        "#             # Process the audio\n",
        "#             input_values = processor(input_waveforms, return_tensors=\"pt\", padding=True).input_values\n",
        "#             input_values = input_values.to(device)\n",
        "\n",
        "#             # Forward pass\n",
        "#             outputs = model(input_values, labels=labels)\n",
        "#             loss = outputs.loss\n",
        "#             logits = outputs.logits\n",
        "\n",
        "#             # Backward pass\n",
        "#             optimizer.zero_grad()\n",
        "#             loss.backward()\n",
        "#             optimizer.step()\n",
        "\n",
        "#             total_loss += loss.item()\n",
        "\n",
        "#             # Accuracy calculation\n",
        "#             preds = torch.argmax(logits, dim=-1)\n",
        "#             correct_preds_train += (preds == labels).sum().item()\n",
        "#             total_train += labels.size(0)\n",
        "\n",
        "#         # Calculate training accuracy\n",
        "#         train_accuracy = correct_preds_train / total_train\n",
        "#         train_accuracies.append(train_accuracy)\n",
        "\n",
        "#         # Validation phase\n",
        "#         model.eval()\n",
        "#         correct_preds_val = 0\n",
        "#         total_val = 0\n",
        "#         with torch.no_grad():\n",
        "#             for batch in val_dataloader:\n",
        "#                 input_waveforms, labels = batch\n",
        "#                 input_waveforms = input_waveforms.squeeze(1).to(device)\n",
        "#                 labels = labels.to(device)\n",
        "\n",
        "#                 input_values = processor(input_waveforms, return_tensors=\"pt\", padding=True).input_values\n",
        "#                 input_values = input_values.to(device)\n",
        "\n",
        "#                 # Predict\n",
        "#                 outputs = model(input_values)\n",
        "#                 logits = outputs.logits\n",
        "#                 preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "#                 correct_preds_val += (preds == labels).sum().item()\n",
        "#                 total_val += labels.size(0)\n",
        "\n",
        "#         # Calculate validation accuracy\n",
        "#         val_accuracy = correct_preds_val / total_val\n",
        "#         val_accuracies.append(val_accuracy)\n",
        "\n",
        "#         # Save the best model\n",
        "#         val_loss = total_loss / len(train_dataloader)\n",
        "#         if val_loss < best_val_loss:\n",
        "#             best_val_loss = val_loss\n",
        "#             save_best_model(model, epoch, best_val_loss)\n",
        "\n",
        "#         # Early stopping\n",
        "#         early_stopping(val_loss)\n",
        "#         if early_stopping.early_stop:\n",
        "#             print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "#             break\n",
        "\n",
        "#         training_losses.append(total_loss / len(train_dataloader))\n",
        "#         validation_losses.append(val_loss)\n",
        "\n",
        "#     return training_losses, validation_losses, train_accuracies, val_accuracies\n",
        "\n",
        "# # Training the model and getting losses and accuracies\n",
        "# training_losses, validation_losses, train_accuracies, val_accuracies = train_model(model, train_dataloader, val_dataloader, optimizer, loss_fn, epochs=10)\n",
        "\n",
        "# # Display Training Loss and Accuracy\n",
        "# display_training_loss_accuracy(training_losses, validation_losses, train_accuracies, val_accuracies, epochs=10)\n",
        "\n",
        "# # Save model4 after training\n",
        "# #model4 = model  # Final model\n"
      ],
      "metadata": {
        "id": "-WNu6serWHjt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498
        },
        "outputId": "a41c2c19-44d2-408b-8cdc-72c5bbdfe567"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/10:   0%|          | 0/26 [00:00<?, ?it/s]<ipython-input-16-c31dac6cb5c4>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  waveform = torch.tensor(self.dataframe.iloc[idx]['Processed Audio'], dtype=torch.float32)\n",
            "<ipython-input-25-90632eff9b7b>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels, dtype=torch.long).to(device)  # Ensure labels are tensors\n",
            "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n",
            "Epoch 1/10:   0%|          | 0/26 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 32, 32000]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-90632eff9b7b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;31m# Training the model and getting losses and accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m \u001b[0mtraining_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;31m# Display Training Loss and Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-90632eff9b7b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, output_attentions, output_hidden_states, return_dict, labels)\u001b[0m\n\u001b[1;32m   2357\u001b[0m         \u001b[0moutput_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_weighted_layer_sum\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2359\u001b[0;31m         outputs = self.wav2vec2(\n\u001b[0m\u001b[1;32m   2360\u001b[0m             \u001b[0minput_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values, attention_mask, mask_time_indices, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1808\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1809\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1810\u001b[0;31m         \u001b[0mextract_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1811\u001b[0m         \u001b[0mextract_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_values)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mconv_layer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_requires_grad\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m                 hidden_states = self._gradient_checkpointing_func(\n\u001b[0m\u001b[1;32m    460\u001b[0m                     \u001b[0mconv_layer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_compile.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dynamo_disable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdisable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\u001b[0m in \u001b[0;36m_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0mprior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m                 \u001b[0m_maybe_set_eval_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprior\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mcheckpoint\u001b[0;34m(function, use_reentrant, context_fn, determinism_check, debug, *args, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m                 \u001b[0;34m\"use_reentrant=False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             )\n\u001b[0;32m--> 489\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mCheckpointFunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m         gen = _checkpoint_without_reentrant_generator(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;31m# See NOTE: [functorch vjp and autograd interaction]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_functorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap_dead_wrappers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_setup_ctx_defined\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, run_function, preserve_rng_state, *args)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/wav2vec2/modeling_wav2vec2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 361\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    362\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    368\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             )\n\u001b[0;32m--> 370\u001b[0;31m         return F.conv1d(\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected 2D (unbatched) or 3D (batched) input to conv1d, but got input of size: [1, 1, 32, 32000]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.2 Define Training Loop\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_dataloader, val_dataloader, optimizer, loss_fn, epochs=3):\n",
        "    model.train()\n",
        "    best_val_loss = float('inf')\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "    train_accuracies = []\n",
        "    val_accuracies = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        correct_preds_train = 0\n",
        "        total_train = 0\n",
        "        print(f\"Epoch {epoch + 1}/{epochs} starting...\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\")):\n",
        "            input_waveforms, labels = batch\n",
        "            input_waveforms = input_waveforms.to(device)  # Ensure correct shape\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Process the audio (ensure shape [batch_size, num_samples])\n",
        "            input_values = processor(input_waveforms, return_tensors=\"pt\", padding=True).input_values\n",
        "            input_values = input_values.squeeze(1)  # Remove unnecessary channel dimension (if present)\n",
        "            input_values = input_values.squeeze(0)  # Remove the extra batch dimension\n",
        "            input_values = input_values.to(device)  # Move to device\n",
        "\n",
        "            # Print the shape of the input values after processing\n",
        "            print(f\"Batch {batch_idx + 1}: Input values shape: {input_values.shape}\")\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_values, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Accuracy calculation\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "            correct_preds_train += (preds == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        train_accuracy = correct_preds_train / total_train\n",
        "        train_accuracies.append(train_accuracy)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        correct_preds_val = 0\n",
        "        total_val = 0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_dataloader:\n",
        "                input_waveforms, labels = batch\n",
        "                input_waveforms = input_waveforms.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Process the audio (ensure shape [batch_size, num_samples])\n",
        "                input_values = processor(input_waveforms, return_tensors=\"pt\", padding=True).input_values\n",
        "                input_values = input_values.squeeze(1)  # Remove unnecessary channel dimension\n",
        "                input_values = input_values.squeeze(0)  # Remove the extra batch dimension\n",
        "                input_values = input_values.to(device)\n",
        "\n",
        "                # Predict\n",
        "                outputs = model(input_values)\n",
        "                logits = outputs.logits\n",
        "                preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "                correct_preds_val += (preds == labels).sum().item()\n",
        "                total_val += labels.size(0)\n",
        "\n",
        "        # Calculate validation accuracy\n",
        "        val_accuracy = correct_preds_val / total_val\n",
        "        val_accuracies.append(val_accuracy)\n",
        "\n",
        "        # Save the best model\n",
        "        val_loss = total_loss / len(train_dataloader)\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            save_best_model(model, epoch, best_val_loss)\n",
        "\n",
        "        # Early stopping\n",
        "        early_stopping(val_loss)\n",
        "        if early_stopping.early_stop:\n",
        "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "            break\n",
        "\n",
        "        training_losses.append(total_loss / len(train_dataloader))\n",
        "        validation_losses.append(val_loss)\n",
        "\n",
        "    return training_losses, validation_losses, train_accuracies, val_accuracies\n"
      ],
      "metadata": {
        "id": "n0BQNMW1n9kR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model and getting losses and accuracies\n",
        "training_losses, validation_losses, train_accuracies, val_accuracies = train_model(model, train_dataloader, val_dataloader, optimizer, loss_fn, epochs=100)\n",
        "\n",
        "# Display Training Loss and Accuracy\n",
        "display_training_loss_accuracy(training_losses, validation_losses, train_accuracies, val_accuracies, epochs=10)\n",
        "\n",
        "# Save model4 after training\n",
        "#model4 = model  # Final model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "3kzRGUmurP2e",
        "outputId": "96a458ad-edb2-4670-9351-2cba05d921ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 starting...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/100:   0%|          | 0/26 [00:00<?, ?it/s]<ipython-input-16-c31dac6cb5c4>:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  waveform = torch.tensor(self.dataframe.iloc[idx]['Processed Audio'], dtype=torch.float32)\n",
            "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1: Input values shape: torch.Size([32, 32000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/100:   4%|â–         | 1/26 [00:01<00:35,  1.42s/it]It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2: Input values shape: torch.Size([32, 32000])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/100:   8%|â–Š         | 2/26 [00:02<00:30,  1.27s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "stack expects each tensor to be equal size, but got [32000] at entry 0 and [2, 32000] at entry 3",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2d61a30cac37>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training the model and getting losses and accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Display Training Loss and Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdisplay_training_loss_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracies\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-d35c6896e75f>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, val_dataloader, optimizer, loss_fn, epochs)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{epochs} starting...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch + 1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0minput_waveforms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0minput_waveforms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_waveforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Ensure correct shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Handle `CustomType` automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \"\"\"\n\u001b[0;32m--> 398\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_collate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             return [\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             return [\n\u001b[0;32m--> 212\u001b[0;31m                 \u001b[0mcollate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             ]  # Backwards compatibility.\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcollate_fn_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0melem_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melem_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcollate_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcollate_fn_map\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_shared\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 272\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [32000] at entry 0 and [2, 32000] at entry 3"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize training performance"
      ],
      "metadata": {
        "id": "07BsWsOBgKLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Display Training Loss and Accuracy\n",
        "def display_training_loss_accuracy(training_losses, validation_losses, train_accuracies, val_accuracies, epochs):\n",
        "    plt.figure(figsize=(20, 4))\n",
        "\n",
        "    # Accuracy Plot\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_accuracies, label='Train Accuracy')\n",
        "    plt.plot(val_accuracies, label='Validation Accuracy')\n",
        "    plt.title('Model Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend(loc='upper left')\n",
        "\n",
        "    # Loss Plot\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(training_losses, label='Train Loss')\n",
        "    plt.plot(validation_losses, label='Validation Loss')\n",
        "    plt.title('Model Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "#display_training_loss_accuracy(training_losses, validation_losses, epochs=10)\n"
      ],
      "metadata": {
        "id": "oi4Sgjqg-l7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Evaluate and Generate Classification Report\n",
        "def evaluate_model(model, dataloader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    true_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_waveforms, labels = batch\n",
        "            input_waveforms = input_waveforms.squeeze(1).to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            input_values = processor(input_waveforms, return_tensors=\"pt\", padding=True).input_values\n",
        "            input_values = input_values.to(device)\n",
        "\n",
        "            # Predict\n",
        "            outputs = model(input_values)\n",
        "            logits = outputs.logits\n",
        "            predicted_labels = torch.argmax(logits, dim=-1)\n",
        "\n",
        "            predictions.extend(predicted_labels.cpu().numpy())\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(true_labels), np.array(predictions)\n",
        "\n",
        "# Evaluate model on validation set\n",
        "true_labels_val, predictions_val = evaluate_model(model, val_dataloader)\n",
        "\n",
        "labels = ['Negative', 'Positive', 'Neutral']\n",
        "print(\"\\nValidation Data Classification Report:\")\n",
        "print(\"--------------------------------------------------------\")\n",
        "print(classification_report(true_labels_val, predictions_val, target_names=labels))"
      ],
      "metadata": {
        "id": "H0__W_U9-l5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jb_19R8H-l2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaKmqp5a7xvl"
      },
      "outputs": [],
      "source": []
    }
  ]
}